{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install imbalanced-learn\n",
    "!pip install --upgrade ipywidgets\n",
    "!pip install optuna\n",
    "!pip install transformers --upgrade\n",
    "!pip install accelerate --upgrade\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the tokenized text from your data_frames\n",
    "all_text = np.concatenate([df_train['text'], df_validation['text'], df_test['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer and convert text to sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(df_train['text'])\n",
    "X_valid_seq = tokenizer.texts_to_sequences(df_validation['text'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(df_test['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences to have the same length\n",
    "max_sequence_length = max(map(len, X_train_seq + X_valid_seq + X_test_seq))\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length, padding='post')\n",
    "X_valid_padded = pad_sequences(X_valid_seq, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the embedding dimension\n",
    "embedding_dim = 50  # Reduced embedding dimension\n",
    "\n",
    "# Define smaller LSTM units\n",
    "lstm_units = 64  # Reduced number of units\n",
    "\n",
    "# Create a Sequential model\n",
    "lstm_model = Sequential()\n",
    "\n",
    "# Add an Embedding layer with reduced dimensions\n",
    "lstm_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "\n",
    "# Add a single LSTM layer with reduced units and no return sequences\n",
    "lstm_model.add(LSTM(lstm_units))\n",
    "\n",
    "# Add a Dropout layer to prevent overfitting\n",
    "lstm_model.add(Dropout(0.3))  # Reduced dropout rate\n",
    "\n",
    "# Add a Dense layer for the final classification, using sigmoid activation for binary classification\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with binary cross-entropy loss and the Adam optimizer\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "586/586 [==============================] - 560938s 957s/step - loss: 0.6938 - accuracy: 0.4946 - val_loss: 0.6990 - val_accuracy: 0.4532\n",
      "Epoch 2/4\n",
      "586/586 [==============================] - 525582s 897s/step - loss: 0.6937 - accuracy: 0.5042 - val_loss: 0.6930 - val_accuracy: 0.5468\n",
      "Epoch 3/4\n",
      "291/586 [=============>................] - ETA: 72:36:57 - loss: 0.6938 - accuracy: 0.4996"
     ]
    }
   ],
   "source": [
    "# Train the LSTM model\n",
    "lstm_model.fit(X_train_padded, y_train, validation_data=(X_valid_padded, y_valid), epochs=4, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model crashed with 4 epoch batch size 8 in the middle of 3rd epoch and thus we executed 5 epoch 16 batch size on the google collab using GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the LSTM model\n",
    "y_valid_pred_lstm = lstm_model.predict_classes(X_valid_padded)\n",
    "valid_accuracy = accuracy_score(y_valid, y_valid_pred_lstm)\n",
    "print(\"Test Accuracy:\", valid_accuracy)\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_valid, y_valid_pred_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the LSTM model\n",
    "y_test_pred_lstm = lstm_model.predict_classes(X_test_padded)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred_lstm)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Clinical BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Load Clinical BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert the datasets to HuggingFace Dataset format\n",
    "train_dataset = Dataset.from_pandas(df_train_bert)\n",
    "test_dataset = Dataset.from_pandas(df_test_bert)\n",
    "val_dataset = Dataset.from_pandas(df_validation_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    if not all(isinstance(item, str) for item in examples['text']):\n",
    "        problematic_items = [item for item in examples['text'] if not isinstance(item, str)]\n",
    "        print(f\"Non-string items: {problematic_items}\")\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the format to torch tensors\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,      # Reduce batch size for memory efficiency\n",
    "    per_device_eval_batch_size=8,       # Keep it similar to per_device_train_batch_size\n",
    "    num_train_epochs=4,                 # Train for 3 epochs\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=1000,                    # Save checkpoints less frequently\n",
    "    eval_steps=1500,                    # Evaluate less frequently\n",
    "    logging_steps=500,                  # Log less frequently\n",
    "    learning_rate=3e-5,                 # Slightly higher learning rate for faster convergence\n",
    "    warmup_steps=300,                   # Gradually warm up the learning rate\n",
    "    weight_decay=0.01,                  # Apply L2 regularization\n",
    "    output_dir='./results',\n",
    "    logging_dir='./logs',\n",
    "    logging_first_step=False,           # No need to log the very first step\n",
    "    gradient_accumulation_steps=4,      # Further reduce memory usage with gradient accumulation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics function for evaluation\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with built-in progress monitoring and logging\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validation set\n",
    "eval_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(\"Validation Results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test dataset\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display the accuracy\n",
    "accuracy = test_results['eval_accuracy']\n",
    "print(f\"Accuracy on test dataset: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# Define a list of hyperparameter combinations to try\n",
    "learning_rates = [1e-5, 3e-5, 1e-4]\n",
    "batch_sizes = [4, 8, 16]\n",
    "num_epochs = [2, 4, 6]\n",
    "weight_decays = [0.0, 0.1, 0.2]\n",
    "\n",
    "hyperparameter_combinations = list(itertools.product(learning_rates, batch_sizes, num_epochs, weight_decays))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through hyperparameter combinations\n",
    "for learning_rate, batch_size, num_train_epochs, weight_decay in hyperparameter_combinations:\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps=1000,\n",
    "        eval_steps=1500,\n",
    "        logging_steps=500,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=300,\n",
    "        weight_decay=weight_decay,\n",
    "        output_dir='./results',\n",
    "        logging_dir='./logs',\n",
    "        logging_first_step=False,\n",
    "        gradient_accumulation_steps=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validation set\n",
    "    eval_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "    print(\"Hyperparameters:\", learning_rate, batch_size, num_train_epochs, weight_decay)\n",
    "    print(\"Validation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test dataset\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "    print(\"Test Results:\", test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
